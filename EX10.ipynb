{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81eed851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fc05c",
   "metadata": {},
   "source": [
    "# 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8f2d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182692</th>\n",
       "      <td>Once you have made a promise, you should keep it.</td>\n",
       "      <td>Une fois que vous avez fait une promesse, vous...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186910</th>\n",
       "      <td>I don't know what it is, but it's something ve...</td>\n",
       "      <td>Je ne sais pas de quoi il s'agit, mais c'est q...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24194</th>\n",
       "      <td>I love you anyway.</td>\n",
       "      <td>Je t'aime quand même.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87297</th>\n",
       "      <td>I gave you what you wanted.</td>\n",
       "      <td>Je vous ai donné ce que vous vouliez.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102629</th>\n",
       "      <td>I had a terrible stomachache.</td>\n",
       "      <td>J'avais un terrible mal d'estomac.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "182692  Once you have made a promise, you should keep it.   \n",
       "186910  I don't know what it is, but it's something ve...   \n",
       "24194                                  I love you anyway.   \n",
       "87297                         I gave you what you wanted.   \n",
       "102629                      I had a terrible stomachache.   \n",
       "\n",
       "                                                      fra  \\\n",
       "182692  Une fois que vous avez fait une promesse, vous...   \n",
       "186910  Je ne sais pas de quoi il s'agit, mais c'est q...   \n",
       "24194                               Je t'aime quand même.   \n",
       "87297               Je vous ai donné ce que vous vouliez.   \n",
       "102629                 J'avais un terrible mal d'estomac.   \n",
       "\n",
       "                                                       cc  \n",
       "182692  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "186910  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "24194   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "87297   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "102629  CC-BY 2.0 (France) Attribution: tatoeba.org #1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec1bd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29129</th>\n",
       "      <td>He is my classmate.</td>\n",
       "      <td>C'est mon camarade de classe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25161</th>\n",
       "      <td>It's cloudy today.</td>\n",
       "      <td>C'est nuageux aujourd'hui.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>That is mine.</td>\n",
       "      <td>C'est à moi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32362</th>\n",
       "      <td>This is irrelevant.</td>\n",
       "      <td>C'est hors sujet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       eng                            fra\n",
       "29129  He is my classmate.  C'est mon camarade de classe.\n",
       "25161   It's cloudy today.     C'est nuageux aujourd'hui.\n",
       "5884         That is mine.                   C'est à moi.\n",
       "32362  This is irrelevant.              C'est hors sujet.\n",
       "51                  Hello!                        Salut !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:33000] # 3만3천개 샘플 사용\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beccfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "\n",
    "sos_token = '<start>'\n",
    "eos_token = '<end>'\n",
    "\n",
    "def preprocessing_sentence (sentence):\n",
    "    # Punctuation\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence\n",
    "                      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cefd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_sentence_decoder (sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fbe6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.eng = lines.eng.apply(lambda x : preprocessing_sentence(x))\n",
    "lines.fra = lines.fra.apply(lambda x : preprocessing_sentence_decoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0fe8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = Tokenizer()   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346c7699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[28, 1], [28, 1], [28, 1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfdf466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 75, 8, 2], [1, 365, 3, 2], [1, 28, 512, 8, 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer()   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 33000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df71428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4671\n",
      "프랑스어 단어장의 크기 : 7454\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4001b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f95511f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 4671\n",
      "프랑스어 단어장의 크기 : 7454\n",
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140e7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d723029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 75, 8], [1, 365, 3], [1, 28, 512, 8]]\n",
      "[[75, 8, 2], [365, 3, 2], [28, 512, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3c0e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da5f19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input = to_categorical(encoder_input)\n",
    "# decoder_input = to_categorical(decoder_input)\n",
    "# decoder_target = to_categorical(decoder_target)\n",
    "# print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "# print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "# print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf93549",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b971f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578da2b4",
   "metadata": {},
   "source": [
    "# 2. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e1996c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_state = 256\n",
    "hidden_state = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d046d729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 인코더에서 사용할 임베딩\n",
    "encoder_inputs = Input(shape=(None, ), name='encoder_input')\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_state, input_length=max_eng_seq_len)(encoder_inputs)\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08fd247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 decoder\n",
    "decoder_inputs = Input(shape=(None, ), name='decoder_input')\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_state)(decoder_inputs)\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dec_emb, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dae0328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d80bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    1195776     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    1908224     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 525312      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7454)   1915678     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 6,070,302\n",
      "Trainable params: 6,070,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1725a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5178 - acc: 0.9290 - val_loss: 0.8062 - val_acc: 0.8799\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5139 - acc: 0.9291 - val_loss: 0.8080 - val_acc: 0.8802\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5111 - acc: 0.9299 - val_loss: 0.8047 - val_acc: 0.8804\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5073 - acc: 0.9302 - val_loss: 0.8075 - val_acc: 0.8787\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5032 - acc: 0.9307 - val_loss: 0.8053 - val_acc: 0.8806\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.5001 - acc: 0.9312 - val_loss: 0.8067 - val_acc: 0.8796\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4974 - acc: 0.9314 - val_loss: 0.8066 - val_acc: 0.8800\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4947 - acc: 0.9319 - val_loss: 0.8082 - val_acc: 0.8809\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4916 - acc: 0.9321 - val_loss: 0.8076 - val_acc: 0.8799\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4890 - acc: 0.9324 - val_loss: 0.8055 - val_acc: 0.8800\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4868 - acc: 0.9325 - val_loss: 0.8078 - val_acc: 0.8804\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4845 - acc: 0.9327 - val_loss: 0.8073 - val_acc: 0.8810\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4824 - acc: 0.9327 - val_loss: 0.8054 - val_acc: 0.8792\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4801 - acc: 0.9329 - val_loss: 0.8037 - val_acc: 0.8797\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4785 - acc: 0.9333 - val_loss: 0.8071 - val_acc: 0.8794\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4758 - acc: 0.9337 - val_loss: 0.8070 - val_acc: 0.8806\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4727 - acc: 0.9342 - val_loss: 0.8060 - val_acc: 0.8808\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4701 - acc: 0.9342 - val_loss: 0.8035 - val_acc: 0.8803\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4674 - acc: 0.9346 - val_loss: 0.8086 - val_acc: 0.8807\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4652 - acc: 0.9349 - val_loss: 0.8075 - val_acc: 0.8792\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4630 - acc: 0.9348 - val_loss: 0.8103 - val_acc: 0.8799\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4614 - acc: 0.9349 - val_loss: 0.8089 - val_acc: 0.8793\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4595 - acc: 0.9350 - val_loss: 0.8076 - val_acc: 0.8803\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4582 - acc: 0.9352 - val_loss: 0.8108 - val_acc: 0.8796\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4565 - acc: 0.9354 - val_loss: 0.8101 - val_acc: 0.8794\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4548 - acc: 0.9357 - val_loss: 0.8091 - val_acc: 0.8791\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4540 - acc: 0.9358 - val_loss: 0.8085 - val_acc: 0.8799\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4532 - acc: 0.9358 - val_loss: 0.8113 - val_acc: 0.8789\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4520 - acc: 0.9360 - val_loss: 0.8124 - val_acc: 0.8791\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4514 - acc: 0.9361 - val_loss: 0.8137 - val_acc: 0.8789\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4499 - acc: 0.9362 - val_loss: 0.8153 - val_acc: 0.8789\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4483 - acc: 0.9362 - val_loss: 0.8138 - val_acc: 0.8786\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4469 - acc: 0.9366 - val_loss: 0.8175 - val_acc: 0.8792\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4453 - acc: 0.9367 - val_loss: 0.8176 - val_acc: 0.8796\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4440 - acc: 0.9368 - val_loss: 0.8213 - val_acc: 0.8783\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4419 - acc: 0.9371 - val_loss: 0.8177 - val_acc: 0.8789\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4406 - acc: 0.9372 - val_loss: 0.8201 - val_acc: 0.8784\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4389 - acc: 0.9372 - val_loss: 0.8194 - val_acc: 0.8793\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4371 - acc: 0.9376 - val_loss: 0.8218 - val_acc: 0.8790\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4360 - acc: 0.9378 - val_loss: 0.8259 - val_acc: 0.8787\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4351 - acc: 0.9377 - val_loss: 0.8254 - val_acc: 0.8794\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4336 - acc: 0.9377 - val_loss: 0.8251 - val_acc: 0.8784\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4320 - acc: 0.9380 - val_loss: 0.8308 - val_acc: 0.8788\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4311 - acc: 0.9383 - val_loss: 0.8314 - val_acc: 0.8786\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4303 - acc: 0.9381 - val_loss: 0.8331 - val_acc: 0.8782\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4296 - acc: 0.9383 - val_loss: 0.8332 - val_acc: 0.8775\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4284 - acc: 0.9385 - val_loss: 0.8349 - val_acc: 0.8790\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4273 - acc: 0.9385 - val_loss: 0.8379 - val_acc: 0.8775\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4253 - acc: 0.9386 - val_loss: 0.8360 - val_acc: 0.8780\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4235 - acc: 0.9390 - val_loss: 0.8412 - val_acc: 0.8775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1359af700>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], \n",
    "          y=decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=30, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef6a0b",
   "metadata": {},
   "source": [
    "# 3. 모델 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0eed957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         1195776   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 1,721,088\n",
      "Trainable params: 1,721,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f485ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = Embedding(fra_vocab_size, 256)(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "77f241b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 256)    1908224     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  525312      embedding_3[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,433,536\n",
      "Trainable params: 2,433,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs2)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80e27878",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0a375df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f117e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + idx2eng[i]+' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09d40b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=fra2idx['<start>']) and i!=fra2idx['<end>']):\n",
    "            temp = temp + idx2fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dd70d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: i m so tired . \n",
      "정답 문장: je suis tellement fatigu . \n",
      "번역기가 번역한 문장:  sens y les les le\n",
      "-----------------------------------\n",
      "입력 문장: tom intervened . \n",
      "정답 문장: tom intervint . \n",
      "번역기가 번역한 문장:  on on mauvais lu\n",
      "-----------------------------------\n",
      "입력 문장: are you guys ok ? \n",
      "정답 문장: a va tout le monde ? \n",
      "번역기가 번역한 문장:  on tellement tellemen\n",
      "-----------------------------------\n",
      "입력 문장: we re rich . \n",
      "정답 문장: nous sommes riches . \n",
      "번역기가 번역한 문장:  avez vrai te te t\n",
      "-----------------------------------\n",
      "입력 문장: who plays golf ? \n",
      "정답 문장: qui joue au golf ? \n",
      "번역기가 번역한 문장:  manger dis dis di\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,77,531,733,2373]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', seq2src(encoder_input_test[seq_index]))\n",
    "    print('정답 문장:', seq2tar(decoder_input_test[seq_index])) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a5dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011f058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d794e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba77cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7ed41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2966fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcae54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ab7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
